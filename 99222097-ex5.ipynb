{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mTDuoVfsStAS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import platform\n",
        "import time\n",
        "import pathlib\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1CgMH9NSxDb",
        "outputId": "77e9b7c3-156b-47bc-824b-592a3faa8303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "filename = '/content/Persian-WikiText-1.txt'\n",
        "text = open(filename, 'r', encoding='utf-8').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xUnvIoiAS2JH"
      },
      "outputs": [],
      "source": [
        "text = text[:1000000]\n",
        "text = text.lower()\n",
        "text = text.replace('\\n', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EgEvQRdTWLIU"
      },
      "outputs": [],
      "source": [
        "# we map each word to a number\n",
        "vocab = sorted(set(text))\n",
        "charToInt = {char: index for index, char in enumerate(vocab)}\n",
        "intToChar = np.array(vocab)\n",
        "textAsInt = np.array([charToInt[char] for char in text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3HvChxAoWLfn"
      },
      "outputs": [],
      "source": [
        "charDataset = tf.data.Dataset.from_tensor_slices(textAsInt) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ovNiRROGWL9C"
      },
      "outputs": [],
      "source": [
        "sequences = charDataset.batch(101, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HUya1HYuWMjd"
      },
      "outputs": [],
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "O1YW9VrZWUrY"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Z0dYjLCfWVFm"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.shuffle(10000).batch(64, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7EOk03pBWWGH"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Embedding(\n",
        "      input_dim=len(vocab),\n",
        "      output_dim=256,\n",
        "      batch_input_shape=[64, None]\n",
        "    ))\n",
        "\n",
        "model.add(tf.keras.layers.LSTM(\n",
        "      units=1024,\n",
        "      return_sequences=True,\n",
        "      stateful=True,\n",
        "      recurrent_initializer=tf.keras.initializers.GlorotNormal()\n",
        "    ))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(len(vocab)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Rbh9WefVWWge"
      },
      "outputs": [],
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
        "      y_true=labels,\n",
        "      y_pred=logits,\n",
        "      from_logits=True\n",
        "    )\n",
        "\n",
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(\n",
        "    optimizer=adam_optimizer,\n",
        "    loss=loss\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "p9OTS_Y_WM_H"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = 'tmp/checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'LSTM_Model_{epoch}')\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSjYwYUnWgjB",
        "outputId": "01027bfc-f485-4b4c-ea1a-2654223a99e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "154/154 [==============================] - 1026s 7s/step - loss: 2.8191\n",
            "Epoch 2/20\n",
            "154/154 [==============================] - 1031s 7s/step - loss: 2.4167\n",
            "Epoch 3/20\n",
            "154/154 [==============================] - 1018s 7s/step - loss: 2.1856\n",
            "Epoch 4/20\n",
            "154/154 [==============================] - 1017s 7s/step - loss: 2.0009\n",
            "Epoch 5/20\n",
            "154/154 [==============================] - 1019s 7s/step - loss: 1.8624\n",
            "Epoch 6/20\n",
            "154/154 [==============================] - 1022s 7s/step - loss: 1.7617\n",
            "Epoch 7/20\n",
            "154/154 [==============================] - 1013s 7s/step - loss: 1.6895\n",
            "Epoch 8/20\n",
            "154/154 [==============================] - 1015s 7s/step - loss: 1.6329\n",
            "Epoch 9/20\n",
            "154/154 [==============================] - 1009s 7s/step - loss: 1.5884\n",
            "Epoch 10/20\n",
            "154/154 [==============================] - 998s 6s/step - loss: 1.5503\n",
            "Epoch 11/20\n",
            "154/154 [==============================] - 1002s 7s/step - loss: 1.5174\n",
            "Epoch 12/20\n",
            "154/154 [==============================] - 1000s 6s/step - loss: 1.4868\n",
            "Epoch 13/20\n",
            "154/154 [==============================] - 1001s 6s/step - loss: 1.4590\n",
            "Epoch 14/20\n",
            "154/154 [==============================] - 1004s 7s/step - loss: 1.4322\n",
            "Epoch 15/20\n",
            "154/154 [==============================] - 998s 6s/step - loss: 1.4069\n",
            "Epoch 16/20\n",
            "154/154 [==============================] - 1002s 6s/step - loss: 1.3817\n",
            "Epoch 17/20\n",
            "154/154 [==============================] - 1002s 6s/step - loss: 1.3575\n",
            "Epoch 18/20\n",
            "154/154 [==============================] - 1002s 7s/step - loss: 1.3331\n",
            "Epoch 19/20\n",
            "154/154 [==============================] - 1004s 7s/step - loss: 1.3072\n",
            "Epoch 20/20\n",
            "154/154 [==============================] - 1006s 7s/step - loss: 1.2840\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "  x=dataset,\n",
        "  epochs=20,\n",
        "  callbacks=[\n",
        "    checkpoint_callback\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK1DZvhVapni"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
